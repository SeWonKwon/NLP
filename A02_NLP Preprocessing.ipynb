{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6417edc8",
   "metadata": {},
   "source": [
    "<a href='https://github.com/SeWonKwon' ><div> <img src ='https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/image_upload/6556674324ed41a289a354258718280d/964e5a8b-75ad-41fc-ae75-0ca66d06fbc7.png' align='left' /> </div></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b286274",
   "metadata": {},
   "source": [
    "# 텍스트 전처리(Text preprocessing) - 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c54c48",
   "metadata": {},
   "source": [
    "자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태라면, 해당 데이터를 사용하고자하는 용도에 맞게 토큰화(tokenization) & 정제(cleaning) & 정규화(normalization)하는 일을 하게 됩니다. 이번 챕터에서는 그 중에서도 토큰화에 대해서 배우도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2871e597",
   "metadata": {},
   "source": [
    "전체 적인 내용정리는 https://wikidocs.net/21698 에서 참조하는것이 좋습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fcd554",
   "metadata": {},
   "source": [
    "## 클렌징"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f2454",
   "metadata": {},
   "source": [
    "텍스트에서 분석에 오히려 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업입니다. 예를 들 HTML, XML 태그나 특정 기호 등을 사전에 제거합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12276c8",
   "metadata": {},
   "source": [
    "## 텍스트 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad221bd",
   "metadata": {},
   "source": [
    "주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 부릅니다. 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의합니다.\n",
    "\n",
    "토큰화의 유형은 문서에서 문장을 분리하는 *문장 토큰화*와 문장에서 단어를 토큰으로 분리하는 *단어 토큰화*로 나눌 수 있습니다. \n",
    "\n",
    "NLTK는 이를 위해 다양한 API를 제공합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2918f",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5021e3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T14:57:26.894855Z",
     "start_time": "2021-09-11T14:57:23.817269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bigne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda2e18",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0195fcb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T14:59:12.275222Z",
     "start_time": "2021-09-11T14:59:12.272218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e8f65d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:09:28.222507Z",
     "start_time": "2021-09-11T15:09:28.205492Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fc5fd6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:09:28.492714Z",
     "start_time": "2021-09-11T15:09:28.477701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "#여러 문장들에 대해 문장별 단어 토큰화 수행. \n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd26049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:06:51.200406Z",
     "start_time": "2021-09-11T15:06:44.367403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520f807",
   "metadata": {},
   "source": [
    "## 스톱 워드 제거(Stop word elimination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ab628",
   "metadata": {},
   "source": [
    "스톱 워드(Stop word)는 분석에 큰 의미가 없는 단어를 지칭합니다.   \n",
    "\n",
    "가령 영어에서 is, the, a, will 등 문장을 구성하는 필수 문법 요소지만 문맥적으로 큰 의미가 없는 단어가 이에 해당합니다.   \n",
    "\n",
    "이것들을 제거하지 않으면 그 빈번함으로 인해 오히려 중요한 단어로 인지될 수 있습니다. 따라서 이 의미 없는 단어를 제거하는 것이 중요한 전처리 작업입니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec9decb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:07:33.206567Z",
     "start_time": "2021-09-11T15:07:33.180545Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bigne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd06796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:07:42.299782Z",
     "start_time": "2021-09-11T15:07:42.276284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "260f2b89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:09:42.404535Z",
     "start_time": "2021-09-11T15:09:42.392817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        #소문자로 모두 변환합니다. \n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275051c0",
   "metadata": {},
   "source": [
    "## 어간 추출(Stemming) and 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dda247",
   "metadata": {},
   "source": [
    "### 어간 추출(Stemming)\n",
    "어간(Stem)을 추출하는 작업을 어간 추출(stemming)이라고 합니다. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있습니다. 다시 말해, 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58d7b639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:15:42.539338Z",
     "start_time": "2021-09-11T15:15:42.524326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023fb2d",
   "metadata": {},
   "source": [
    "### 표제어 추출(Lemmatization)\n",
    "표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어' 정도의 의미를 갖습니다. 표제어 추출은 단어들로부터 표제어를 찾아가는 과정입니다. 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단합니다. 예를 들어서 am, are, is는 서로 다른 스펠링이지만 그 뿌리 단어는 be라고 볼 수 있습니다. 이 때, 이 단어들의 표제어는 be라고 합니다.\n",
    "\n",
    "표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것입니다. 형태소란 '의미를 가진 가장 작은 단위'를 뜻합니다. 그리고 형태학(morphology)이란, 형태소로부터 단어들을 만들어가는 학문을 뜻합니다.\n",
    "\n",
    "형태소는 두 가지 종류가 있습니다. 각각 어간(stem)과 접사(affix)입니다.\n",
    "1) 어간(stem)\n",
    ": 단어의 의미를 담고 있는 단어의 핵심 부분.\n",
    "\n",
    "2) 접사(affix)\n",
    ": 단어에 추가적인 의미를 주는 부분.\n",
    "\n",
    "형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업을 말합니다. 가령, cats라는 단어에 대해 형태학적 파싱을 수행한다면, 형태학적 파싱은 결과로 cat(어간)와 -s(접사)를 분리합니다. 꼭 두 가지로 분리되지 않는 경우도 있습니다. 단어 fox는 형태학적 파싱을 한다고 하더라도 더 이상 분리할 수 없습니다. fox는 독립적인 형태소이기 때문입니다. 이와 유사하게 cat 또한 더 이상 분리되지 않습니다.\n",
    "\n",
    "NLTK에서는 표제어 추출을 위한 도구인 WordNetLemmatizer를 지원합니다. 이를 통해 표제어 추출을 실습해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe55a7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:16:38.073065Z",
     "start_time": "2021-09-11T15:16:36.906987Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bigne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f528383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:16:57.730053Z",
     "start_time": "2021-09-11T15:16:57.715523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n=WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', \n",
    "       'going', 'love', 'lives', 'fly', 'dies', \n",
    "       'watched', 'has', 'starting']\n",
    "\n",
    "print([n.lemmatize(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fee8584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:17:57.500542Z",
     "start_time": "2021-09-11T15:17:57.484987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb03f9cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:18:04.011465Z",
     "start_time": "2021-09-11T15:18:04.004456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3b78a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T15:18:08.673220Z",
     "start_time": "2021-09-11T15:18:08.659207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78426ff",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "\n",
    "* <a href='https://github.com/SeWonKwon' ><div> <img src ='https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/image_upload/6556674324ed41a289a354258718280d/964e5a8b-75ad-41fc-ae75-0ca66d06fbc7.png' align='left' /> </div></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "* 파이썬 머신러닝 완벽 가이드, 권철민\n",
    "* [이수안 컴퓨터 연구소](http://suanlab.com/)\n",
    "* [딥 러닝을 이용한 자연어 처리 입분](https://wikidocs.net/21693)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
